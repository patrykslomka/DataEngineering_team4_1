{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:01:58.969671Z",
     "start_time": "2024-10-11T18:01:35.115652Z"
    }
   },
   "cell_type": "code",
   "source": "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\ google-cloud-aiplatform",
   "id": "892d94ced0869014",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 2.0.0 Requires-Python >=3.7.0,<3.12.0; 2.0.0-beta.14 Requires-Python >=3.7.0,<3.12.0; 2.0.0-beta.17 Requires-Python >=3.7.0,<3.12.0; 2.0.0-rc.1 Requires-Python >=3.7.0,<3.12.0; 2.0.0-rc.2 Requires-Python >=3.7.0,<3.12.0; 2.0.0b15 Requires-Python >=3.7.0,<3.12.0; 2.0.0b16 Requires-Python >=3.7.0,<3.12.0; 2.0.0b2 Requires-Python >=3.7.0,<3.12.0; 2.0.0b3 Requires-Python >=3.7.0,<3.12.0; 2.0.0b4 Requires-Python >=3.7.0,<3.12.0; 2.0.0b5 Requires-Python >=3.7.0,<3.12.0; 2.0.1 Requires-Python >=3.7.0,<3.12.0; 2.1.0 Requires-Python >=3.7.0,<3.12.0; 2.1.1 Requires-Python >=3.7.0,<3.12.0; 2.1.2 Requires-Python >=3.7.0,<3.12.0; 2.1.3 Requires-Python >=3.7.0,<3.12.0; 2.10.0 Requires-Python >=3.7.0,<3.12.0; 2.11.0 Requires-Python <3.12.0,>=3.7.0; 2.12.0 Requires-Python <3.12.0,>=3.7.0; 2.13.0 Requires-Python <3.12.0,>=3.7.0; 2.13.1 Requires-Python <3.12.0,>=3.7.0; 2.14.0 Requires-Python <3.12.0,>=3.8.0; 2.14.1 Requires-Python <3.12.0,>=3.8.0; 2.15.0 Requires-Python <3.12.0,>=3.8.0; 2.16.0 Requires-Python <3.12.0,>=3.8.0; 2.16.1 Requires-Python <3.12.0,>=3.8.0; 2.17.0 Requires-Python <3.12.0,>=3.8.0; 2.2.0 Requires-Python >=3.7.0,<3.12.0; 2.3.0 Requires-Python >=3.7.0,<3.12.0; 2.3.1 Requires-Python >=3.7.0,<3.12.0; 2.4.0 Requires-Python >=3.7.0,<3.12.0; 2.4.1 Requires-Python >=3.7.0,<3.12.0; 2.5.0 Requires-Python >=3.7.0,<3.12.0; 2.6.0 Requires-Python >=3.7.0,<3.12.0; 2.7.0 Requires-Python >=3.7.0,<3.12.0; 2.8.0 Requires-Python >=3.7.0,<3.12.0; 2.9.0 Requires-Python >=3.7.0,<3.12.0\n",
      "ERROR: Could not find a version that satisfies the requirement google-cloud-pipeline-components>2 (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8.dev0, 0.1.8, 0.1.9.dev0, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.3.1, 1.0.dev2, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5.dev0, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.0.9, 1.0.10, 1.0.11, 1.0.12, 1.0.13, 1.0.14, 1.0.15, 1.0.16, 1.0.17, 1.0.18, 1.0.19, 1.0.20, 1.0.21, 1.0.22, 1.0.23, 1.0.24, 1.0.25, 1.0.26, 1.0.27, 1.0.28, 1.0.29, 1.0.30, 1.0.31, 1.0.32, 1.0.33, 1.0.34, 1.0.35, 1.0.36, 1.0.37, 1.0.38, 1.0.39, 1.0.40, 1.0.41, 1.0.42, 1.0.43, 1.0.44, 1.0.45, 2.0.0b0, 2.0.0b1)\n",
      "ERROR: No matching distribution found for google-cloud-pipeline-components>2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in c:\\users\\patry\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:03:24.078400Z",
     "start_time": "2024-10-11T18:03:15.845368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ],
   "id": "51eb64b820bf03a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.9.0\n",
      "google-cloud-aiplatform==1.70.0\n",
      "google_cloud_pipeline_components version: 1.0.33\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T18:03:55.525370Z",
     "start_time": "2024-10-11T18:03:55.508549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:04:16.944994Z",
     "start_time": "2024-10-11T18:04:16.931394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECT_ID = \"hip-lightning-435508-s1\"  # Replace with your Google Cloud project ID\n",
    "REGION = \"us-central1\"  # Adjust the region as needed\n",
    "PIPELINE_ROOT = \"gs://mlops_team4_de2024\"  # Replace with your GCS bucket URI\n"
   ],
   "id": "8512bd11efb06fa2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:04:17.961460Z",
     "start_time": "2024-10-11T18:04:17.939770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the AI platform\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ],
   "id": "d525cf86f81d4b65",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:08:25.286782Z",
     "start_time": "2024-10-11T18:08:25.260865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Ingestion\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''Download data from GCS.'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    " \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        logging.info(f\"Initializing storage client for project: {project_id}\")\n",
    "        client = storage.Client(project=project_id)\n",
    "\n",
    "        # Access the bucket and blob\n",
    "        logging.info(f\"Accessing bucket: {bucket}, file: {file_name}\")\n",
    "        bucket = client.bucket(bucket)\n",
    "        blob = bucket.blob(file_name)\n",
    "        \n",
    "        # Download the file\n",
    "        file_path = dataset.path + \".csv\"\n",
    "        logging.info(f\"Downloading file to: {file_path}\")\n",
    "        blob.download_to_filename(file_path)\n",
    "        logging.info('Downloaded Data successfully!')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in download_data: {str(e)}\")\n",
    "        raise e  # Re-raise the error to capture it in logs"
   ],
   "id": "eb3edd6d65300d9b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:12:17.333383Z",
     "start_time": "2024-10-11T18:12:17.321193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Linear Regression\n",
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Train a Linear Regression model and return performance metrics'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    import logging \n",
    "    import pickle  \n",
    "    from typing import NamedTuple\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    logging.info(f\"Columns in dataset: {df.columns}\")\n",
    "\n",
    "    # Split dataset into features and target variable\n",
    "    X = df.drop(columns=['Median_House_Value'])  # Replace with your target column\n",
    "    y = df['Median_House_Value']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "    # Train the model\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(x_train, y_train)\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = model_lr.predict(x_test)\n",
    "    metrics_dict = {\n",
    "        \"mean_absolute_error\": mean_absolute_error(y_test, predictions),\n",
    "        \"mean_squared_error\": mean_squared_error(y_test, predictions),\n",
    "    }\n",
    "    logging.info(f\"Metrics: {metrics_dict}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_lr, f)   \n",
    "\n",
    "    return NamedTuple('outputs', metrics=dict)(metrics_dict)"
   ],
   "id": "1dda90478c019b4f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:15:20.530630Z",
     "start_time": "2024-10-11T18:15:20.513634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Prediction\n",
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_lr(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    \"\"\"Predict house prices using the trained Linear Regression model.\"\"\"\n",
    "    import pandas as pd\n",
    "    import pickle  \n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the features dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    \n",
    "    # Load the saved model\n",
    "    filename = model.path + \".pkl\"\n",
    "    model_lr = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    # Select features for prediction (using the actual feature names)\n",
    "    xNew = df[[\n",
    "        'Median_Income', 'Median_Age', 'Tot_Rooms', 'Tot_Bedrooms', 'Population',\n",
    "        'Households', 'Latitude', 'Longitude', 'Distance_to_coast', \n",
    "        'Distance_to_LA', 'Distance_to_SanDiego', 'Distance_to_SanJose',\n",
    "        'Distance_to_SanFrancisco'\n",
    "    ]]\n",
    "\n",
    "    # Make predictions\n",
    "    df['predicted_price'] = model_lr.predict(xNew)\n",
    "    logging.info(f\"Predictions: {df['predicted_price'].tolist()}\")\n",
    "\n",
    "    # Save results to the output dataset\n",
    "    df.to_csv(results.path, index=False, encoding='utf-8-sig')"
   ],
   "id": "92011a06389e555f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:15:56.444667Z",
     "start_time": "2024-10-11T10:15:56.425730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Component\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_regression_model(features: Input[Dataset], model: Output[Model]):\n",
    "    '''Train a regression model using Linear Regression.'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression        \n",
    "    import pickle \n",
    "    \n",
    "    # Load the training data\n",
    "    data = pd.read_csv(features.path+\".csv\")\n",
    "    \n",
    "    # Train a Linear Regression model\n",
    "    model_lr = LinearRegression()\n",
    "    X = data.drop('Median_House_Value', axis=1)\n",
    "    y = data['Median_House_Value']\n",
    "    model_lr.fit(X, y)\n",
    "\n",
    "    # Save the model to the specified path\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_lr, file)   "
   ],
   "id": "7964c01b19ff399b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:13:39.857013Z",
     "start_time": "2024-10-11T18:13:39.827724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Evaluation\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\", \"numpy\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def evaluate_model(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    '''Evaluate the trained regression model.'''\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    # Load test data and model\n",
    "    data = pd.read_csv(test_set.path + \".csv\")\n",
    "    X_test = data.drop('Median_House_Value', axis=1)\n",
    "    y_test = data['Median_House_Value']\n",
    "    \n",
    "    model_file = model.path + \".pkl\"\n",
    "    loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Log metrics\n",
    "    metrics.log_metric(\"mean_absolute_error\", mae)\n",
    "    metrics.log_metric(\"root_mean_squared_error\", rmse)"
   ],
   "id": "2a5831123f80d0c3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:13:51.626094Z",
     "start_time": "2024-10-11T18:13:51.609273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model upload\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    \"\"\"Upload the trained model to Google Cloud Storage (GCS).\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        client = storage.Client(project=project_id)\n",
    "\n",
    "        # Get the bucket and create a blob for the model\n",
    "        bucket = client.bucket(model_repo)\n",
    "        model_file_name = \"lr_model.pkl\"  # Assuming a linear regression model saved as a pickle file\n",
    "        blob = bucket.blob(model_file_name)\n",
    "\n",
    "        \n",
    "        # Upload the model file\n",
    "        blob.upload_from_filename(f\"{model.path}{model.metadata['file_type']}\")\n",
    "        \n",
    "        logging.info(f\"Saved the model to GCP bucket: {model_repo} as {model_file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading model to GCS: {str(e)}\")"
   ],
   "id": "2867333bc705bded",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:17:18.041296Z",
     "start_time": "2024-10-11T18:17:17.984382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary modules\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "# Define your pipeline function\n",
    "@dsl.pipeline(\n",
    "    name=\"california-housing-pipeline\"\n",
    ")\n",
    "def california_housing_pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str):\n",
    "    \n",
    "    # Step 1: Download training data\n",
    "    train_data_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    "    # Step 2: Train the model\n",
    "    train_op = train_lr(\n",
    "        features=train_data_op.outputs[\"dataset\"]\n",
    "    )\n",
    "\n",
    "    # Step 3: Download test data\n",
    "    test_data_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    )\n",
    "\n",
    "    # Step 4: Make predictions using the trained model\n",
    "    predict_op = predict_lr(\n",
    "        model=train_op.outputs['out_model'],\n",
    "        features=test_data_op.outputs['dataset']\n",
    "    )\n",
    "\n",
    "    # Step 5: Upload the predicted results\n",
    "    upload_op = upload_model_to_gcs(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,\n",
    "        model=train_op.outputs['out_model']\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=california_housing_pipeline,\n",
    "    package_path='california_housing_pipeline.yaml'\n",
    ")"
   ],
   "id": "c90eb2074f60c4d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T18:33:07.674534Z",
     "start_time": "2024-10-11T18:32:55.743852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "# You can set this in your environment or use the following command in the terminal:\n",
    "# export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/service-account-file.json\"\n",
    "\n",
    "# Initialize AI Platform\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"california-housing-pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"california_housing_pipeline.yaml\",\n",
    "    pipeline_root=\"gs://mlops_team4_de2024/artifacts\",  # Update with a valid GCS path\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'data_bucket': 'gs://mlops_team4_de2024',\n",
    "        'trainset_filename': 'train_data.csv',\n",
    "        'testset_filename': 'test_data.csv',\n",
    "        'model_repo': 'mlops_team4_de2024'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run the pipeline job\n",
    "job.run()"
   ],
   "id": "f18703198d252c0f",
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDefaultCredentialsError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 15\u001B[0m\n\u001B[0;32m      9\u001B[0m aip\u001B[38;5;241m.\u001B[39minit(\n\u001B[0;32m     10\u001B[0m     project\u001B[38;5;241m=\u001B[39mPROJECT_ID,  \u001B[38;5;66;03m# Replace with your actual Google Cloud project ID\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     location\u001B[38;5;241m=\u001B[39mREGION,     \u001B[38;5;66;03m# Replace with the appropriate region, e.g., 'us-central1'\u001B[39;00m\n\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Prepare the pipeline job\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m job \u001B[38;5;241m=\u001B[39m \u001B[43maip\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPipelineJob\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisplay_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcalifornia-housing-pipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Change to a relevant name for your project\u001B[39;49;00m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_caching\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemplate_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcalifornia_housing_pipeline.yaml\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Use the compiled pipeline YAML file name\u001B[39;49;00m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpipeline_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPIPELINE_ROOT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Path to your GCS bucket for storing pipeline artifacts\u001B[39;49;00m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mREGION\u001B[49m\u001B[43m,\u001B[49m\u001B[43m               \u001B[49m\u001B[38;5;66;43;03m# Ensure the region matches where your resources are located\u001B[39;49;00m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparameter_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mproject_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mPROJECT_ID\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                  \u001B[49m\u001B[38;5;66;43;03m# Ensure to use your project ID \u001B[39;49;00m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata_bucket\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgs://mlops_team4_de2024\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Update with your data bucket name\u001B[39;49;00m\n\u001B[0;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrainset_filename\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain_data.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# Ensure the file exists in your GCS bucket\u001B[39;49;00m\n\u001B[0;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtestset_filename\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest_data.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Ensure the file exists in your GCS bucket\u001B[39;49;00m\n\u001B[0;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_repo\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmlops_team4_de2024\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# Update with your model bucket name\u001B[39;49;00m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Run the pipeline job\u001B[39;00m\n\u001B[0;32m     31\u001B[0m job\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:219\u001B[0m, in \u001B[0;36mPipelineJob.__init__\u001B[1;34m(self, display_name, template_path, job_id, pipeline_root, parameter_values, input_artifacts, enable_caching, encryption_spec_key_name, labels, credentials, project, location, failure_policy)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels:\n\u001B[0;32m    217\u001B[0m     utils\u001B[38;5;241m.\u001B[39mvalidate_labels(labels)\n\u001B[1;32m--> 219\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mproject\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproject\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcredentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcredentials\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent \u001B[38;5;241m=\u001B[39m initializer\u001B[38;5;241m.\u001B[39mglobal_config\u001B[38;5;241m.\u001B[39mcommon_location_path(\n\u001B[0;32m    222\u001B[0m     project\u001B[38;5;241m=\u001B[39mproject, location\u001B[38;5;241m=\u001B[39mlocation\n\u001B[0;32m    223\u001B[0m )\n\u001B[0;32m    225\u001B[0m \u001B[38;5;66;03m# this loads both .yaml and .json files because YAML is a superset of JSON\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\aiplatform\\base.py:1234\u001B[0m, in \u001B[0;36mVertexAiResourceNounWithFutureManager.__init__\u001B[1;34m(self, project, location, credentials, resource_name)\u001B[0m\n\u001B[0;32m   1217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m   1218\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1219\u001B[0m     project: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1222\u001B[0m     resource_name: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1223\u001B[0m ):\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Initializes class with project, location, and api_client.\u001B[39;00m\n\u001B[0;32m   1225\u001B[0m \n\u001B[0;32m   1226\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1232\u001B[0m \u001B[38;5;124;03m        resource_name(str): A fully-qualified resource name or ID.\u001B[39;00m\n\u001B[0;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1234\u001B[0m     \u001B[43m_VertexAiResourceNounPlus\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproject\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproject\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcredentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcredentials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresource_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresource_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1240\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1241\u001B[0m     FutureManager\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\aiplatform\\base.py:558\u001B[0m, in \u001B[0;36mVertexAiResourceNoun.__init__\u001B[1;34m(self, project, location, credentials, resource_name)\u001B[0m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject \u001B[38;5;241m=\u001B[39m project \u001B[38;5;129;01mor\u001B[39;00m initializer\u001B[38;5;241m.\u001B[39mglobal_config\u001B[38;5;241m.\u001B[39mproject\n\u001B[0;32m    557\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocation \u001B[38;5;241m=\u001B[39m location \u001B[38;5;129;01mor\u001B[39;00m initializer\u001B[38;5;241m.\u001B[39mglobal_config\u001B[38;5;241m.\u001B[39mlocation\n\u001B[1;32m--> 558\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcredentials \u001B[38;5;241m=\u001B[39m credentials \u001B[38;5;129;01mor\u001B[39;00m \u001B[43minitializer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglobal_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcredentials\u001B[49m\n\u001B[0;32m    560\u001B[0m appended_user_agent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m base_constants\u001B[38;5;241m.\u001B[39mUSER_AGENT_SDK_COMMAND:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\aiplatform\\initializer.py:371\u001B[0m, in \u001B[0;36m_Config.credentials\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    369\u001B[0m logging_warning_filter \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mLoggingFilter(logging\u001B[38;5;241m.\u001B[39mWARNING)\n\u001B[0;32m    370\u001B[0m logger\u001B[38;5;241m.\u001B[39maddFilter(logging_warning_filter)\n\u001B[1;32m--> 371\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_project_as_env_var_or_google_auth_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    372\u001B[0m credentials \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_credentials\n\u001B[0;32m    373\u001B[0m logger\u001B[38;5;241m.\u001B[39mremoveFilter(logging_warning_filter)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\aiplatform\\initializer.py:108\u001B[0m, in \u001B[0;36m_Config._set_project_as_env_var_or_google_auth_default\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    105\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_project \u001B[38;5;241m=\u001B[39m project\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_credentials \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api_key:\n\u001B[1;32m--> 108\u001B[0m     credentials, _ \u001B[38;5;241m=\u001B[39m \u001B[43mgoogle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_credentials \u001B[38;5;241m=\u001B[39m credentials\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\auth\\_default.py:693\u001B[0m, in \u001B[0;36mdefault\u001B[1;34m(scopes, request, quota_project_id, default_scopes)\u001B[0m\n\u001B[0;32m    685\u001B[0m             _LOGGER\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    686\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo project ID could be determined. Consider running \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    687\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`gcloud config set project` or setting the \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    688\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    689\u001B[0m                 environment_vars\u001B[38;5;241m.\u001B[39mPROJECT,\n\u001B[0;32m    690\u001B[0m             )\n\u001B[0;32m    691\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m credentials, effective_project_id\n\u001B[1;32m--> 693\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001B[1;31mDefaultCredentialsError\u001B[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
